

## Bias and Vaariance

![Bias and variance](/Notes/6.%20Tuning/Bias%20And%20variance.png)

![High Bias and High Variance](/Notes/6.%20Tuning/high%20bias%20and%20variance.png)


### How to fix Bias and Variance?

![Steps to Fix](/Notes/6.%20Tuning/steps%20to%20fix.png)


## Regularization

![Regularization](/Notes/6.%20Tuning/Regulariztion.png)

![Regularization 2](/Notes/6.%20Tuning/regularization%202.png)


## Dropout

![Dropout](/Notes/6.%20Tuning/Dropout.png)


## Normalization

![Normalization](/Notes/6.%20Tuning/NOrmalization.png)

If we use unnormalized x, we get a cost function that is more like a swuished out bar. The gradient descent oscillates back and forth a lot so the learning rate has to be very low.

## Vanishing or exploding gradients

When we have a very deep neiral networks with alinear activationg function for each layer, if the weigts of each layer was a little larger than the identity matrix, each subsequent layer would increase the value for Z, this leads to an exponential increase in the value of y^. This is called **Exploding Gradient**.

Similarly, if the weights were a littel smaller than identity matrix, the value for y^ will decrease exponentially. This is called **Variation Gradient**.

This leads to an increase or decrease exponentially for the gradients sent back. this makes training difficult.

### Solution:
 #### Weight initialzation
 In this method we set the initial values of weights close to 1, so the porblem of exploding or vanishing decreses.
 
 ![Weight initialization](/Notes/6.%20Tuning/Weight%20initialization.png)


## Gradient checking

This allows to get an idea for how accurate our logic involving forward propagation adn backpropagation is. If the th evalues of approx d(theta) and actual d(theta) dont match, we can assume that we have a bug somewhere.

![Gradient Checking](/Notes/6.%20Tuning/Gradient%20checking.png)


## Mini-Batch Gradient descent

Batch gradient referes to when we use the entire batch at the same time to train the network.

Mini-batch graddient descent refers to when we divide the batch into multiple mini-batches (call them X{t} where t refers to the batch number).

This is much faster than batch gradient descent.

![Graph mini batch](/Notes/6.%20Tuning/Mini%20batch%20gradient%20descent%20graph.png)

Batch size:

![Mini batch size](/Notes/6.%20Tuning/mini%20batch%20size.png)

## Exponentially weighted average

![Exponentially weighted average](/Notes/6.%20Tuning/Exponentially%20weighted%20average.png)

When we actually implement moving average and B is 0.98, the curve startes lower than expected. see the green curve (expected) and the purple curve (actual).

![bias correctiom](/Notes/6.%20Tuning/bias%20correction.png)

Lets see how to fix it.

![Bias correction](/Notes/6.%20Tuning/bias%20correction%202.png)

## Gradient Descent with Momentum

We want oscillations to be faster in the direction towards the minima adn we want the oscillations to be slower in the direction perpendicular to the minima.

![gradient descent with momentum](/Notes/6.%20Tuning/gradient%20descent%20with%20momentum.png)


## RMSprop

It can also speed up gradient descent.

![RMSprop](/Notes/6.%20Tuning/RMSprop.png)

To ensure the denominators are not zero, we sometimes add a very small value (epsilon).

## Adam Optimization Algorithm

It combines momentum and RMSprop.

ADAM stands for Adaptive moment estimation.

![Adam Optimization](/Notes/6.%20Tuning/Adam%20Optimization.png)

## Learning rate decay

![Learning Rate Decay](/Notes/6.%20Tuning/Learning%20rate%20decay.png)

![Learning rate decay other methods](/Notes/6.%20Tuning/Learning%20rat%20edecay%20other%20methods.png)

## Problems with Local Optima

The optima are not concave points strictly, they can also be saddle points (shaped like a saddle).

Also plateaus can exist which a huge region where slope stays close to zero for large space.
They can make learning slow.


## Usin Appropriate Scale to pick hyperparameters

![logarithmic](/Notes/6.%20Tuning/hyperparameter%20selection%20random%2C%20logarithmic.png)

![Hyperparameters for exponentially weighted averages](/Notes/6.%20Tuning/Hyperparameters%20for%20exponentially%20weighted%20averages.png)

When Beta is close to 1, small changes can have large effects. SO the sampling process is made more dense when Beta is close to 1.

## Implementing Batch Norm

![Implementing Batch Norm](/Notes/6.%20Tuning/Implementing%20Batch%20Norm.png)

![Batch norm to a network](/Notes/6.%20Tuning/Batch%20Norm%20to%20a%20netwrok.png)

![Batch Norm for Mini batches](/Notes/6.%20Tuning/BAtch%20norm%20for%20mini%20batches.png)

We can ignore db here beacuse due to batch norm beta is added which takes care of db.

![Batch Norm Gradient Descent](/Notes/6.%20Tuning/batch%20norm%20gradient%20descent.png)

Batch Norm works because it makes weights in the later layers more robust to chnages than the layers in earlier layers.

Batch Norm also has a small regularization effect.

(Little doubts, go through once again)

## Softmax

![Softmax](/Notes/6.%20Tuning/Softmax.png)

![Loss function](/Notes/6.%20Tuning/Loss%20function%20for%20softmax.png)

![Gradient descent](/Notes/6.%20Tuning/Gradient%20descent%20for%20softmax.png)

While using frameworks, we dont need to worry about back prop, it is taken care of by the framework.