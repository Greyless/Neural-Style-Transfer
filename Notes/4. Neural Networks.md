# Neural Networks

![Neural Network](/Notes/4.%20Neural%20Networks/Neural%20Netwrok.png)

Input layer is numebred layer 0. Its not counted when counting number of layers.

Each node has different weights associated with it, so instead of a 1d matrix as seen in Logistic Regression, we have a 2d matrix where each row in the transpose corresponds to the weights for a different node.

![Weights matix](/Notes/4.%20Neural%20Networks/Weights%20matrix.png)

The matrices for Z and A will also be 2d. Each column in the matrix will correspond to different training example and each row will correspond to the nodes for the same example.

![Z and A 2d matrix](/Notes/4.%20Neural%20Networks/Z%20and%20A%20matrices.png)

## Activation Function

We have been using sigmoid but other have applications too.

There is also **tanh** function which naps from m-1 to 1. It is just a sigmoid function but shifted.
Tanh is almost always superior, one exception is the output layer where we want data to be mapped between 0 to 1.

One problem with both these functions is that as the input value reached very high negative or positive values, the slope tends to zero so the gradient descent decreases.

Another popular function is ReLU.

![ReLU](/Notes/4.%20Neural%20Networks/ReLU.png)

The derivative of this function is 1 when value is greater than 0 and the derivative is 0 when the value is smaller than 0.

As a rule of thumb, when we want binary classification (output layer), we use Signmoid or else ReLU is an increasingly popular choice for an activation function.

![Activation Functions](/Notes/4.%20Neural%20Networks/Activation%20functions.png)


**Why do we need activation functions?**

If we elminate activation functions, we will just have linear functions no matter how large our model is. FOr eg, in a 2 layer model, if our hidden layer is linear and our output layer uses sigmoid function, it is no different than Logistic Regression. The hidden layer is useless since the composition of 2 or more linear fundtions is a linear function.

Linear activation functions could be usefull when we are doing regression, eg to predict pricing of house which takes values other than 0 to 1.

Sigmoid derivative:

![SIgmoid Derivative](/Notes/4.%20Neural%20Networks/Sigmoid%20Derivative.png)

Tanh derivative:

![Tanh Derivative](/Notes/4.%20Neural%20Networks/tanh%20derivative.png)

ReLU derivative:

![ReLU derivative](/Notes/4.%20Neural%20Networks/ReLU%20derivative.png)


The formulas remain the same except the dimension of w,b,A,Z,dw,db, etc changes to accomodate all the nodes.

# Random Initialization

For logistic Regression, initializing all the weights to 0 worked but it wont work for a neural network.

This is because when all weights are 0, activation for 2 different nodes become 0 and subsequently so do the dw1 and dw2. This leads to the new weights being equal. This means no matter how much we optimize, these nodes will compute exactly the same.

The solution is to initialize the parameters randomly.

We usually take a random value and multiply it by 0.01 since we usually want a small starting value as we know larger values on the sigmoid or tanh functions lead to smaller and smaller derivatives.