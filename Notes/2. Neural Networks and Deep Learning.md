# Neural Networks

[Andre NG course 1](https://youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0 "Andrew NG course 1")

Neural Networks use supervised learning where the ntwork needs to be trained using predefined, labeled data so the netwrok is tuned to make predictions. The performance of the network depends upon the complexity and scale of network and also the amount of data used to train it.

![Performance vs Data](/Notes/Neural%20Networks%20and%20Deep%20learning%20images/Data%20vs%20Performance.png "Performance vs Data")

We have also seen algorthmic innovations in recent years that make the neural networks faster and better.

For Neural netowrks, we use binary classification, where 1 means true and 0 means false.

For eg, if a cat image was to be identified, a 1 would mean it is a cat image and a 0 would mean it is not a cat image.

For training, lets say our data set includes m cases and each case has nx data, then our input is a matrix of dimensions (nx, m) where every case is its own column. A matix of dimension (m, nx) can also be used but the former is easier to implement
 
![Input matrix](/Notes/Neural%20Networks%20and%20Deep%20learning%20images/input%20matrix.png "Input matrix")

For output, we need a single output for each case {0,1} hence our output set will be a matrix of dimension (1,m).
 
![Ouput Matrix](/Notes/Neural%20Networks%20and%20Deep%20learning%20images/output%20matrix.png)

## Logistic Regression

The output tells us the probability of a certain case. Therefore, the out y^ lies between 0 and 1.

We can use logistic regression only but that doesn't necessarily give us values between 0 and 1. Hence, we also make use of the sigmoid function. It takes input of any real number and maps it between 0 and 1.

![Sigmoid Function](/Notes/Neural%20Networks%20and%20Deep%20learning%20images/Sigmoid%20function.png "Sigmoid Function")

The function used for this is as follows

![y^ Function](/Notes/Neural%20Networks%20and%20Deep%20learning%20images/function%20used.png)

## Loss Function

We need to define a loss function that gives a measure of how good our network is. Its parameters are the output expected and the output recieved.    L(y^,y).

We can use the squared difference but for easier optimization, we use the following loss function

**L(y^,y) = -[ylog(y^)+(1-y)log(1-y^)]**

The loss function is defined for one trainig example but we need a measure of how well the netwrok performs for all the training examples. Hence, we define a **Cost Function**.

![Cost function](/Notes/Neural%20Networks%20and%20Deep%20learning%20images/cost%20function.png "Cost function")

We want to fine the w,b for which the cost function is minimum.

## Gradient Descent

Our cost function using log has made the function a convex function. On graphing, we get a single bowl. We get a single minima instead of multiple local minima.

![Cost function graph](/Notes/Neural%20Networks%20and%20Deep%20learning%20images/cost%20function%20graph.png "COst function Graph")

The way gradient descent works is that we take a random value of w and b and then we step down in the steepest downhil direction. After multiple iterations we reach the global optimum or atleast close to it.

![Gradient Descent](/Notes/Neural%20Networks%20and%20Deep%20learning%20images/Gradient%20descent.png "Gradient Descent")

