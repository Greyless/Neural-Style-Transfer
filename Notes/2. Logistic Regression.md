# Neural Networks

[Andrew NG course 1](https://youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0 "Andrew NG course 1")

Neural Networks use supervised learning where the ntwork needs to be trained using predefined, labeled data so the netwrok is tuned to make predictions. The performance of the network depends upon the complexity and scale of network and also the amount of data used to train it.

![Performance vs Data](/Notes/2.%20Logistic%20Regression/Data%20vs%20Performance.png"Performance vs Data")

We have also seen algorthmic innovations in recent years that make the neural networks faster and better.

For Neural netowrks, we use binary classification, where 1 means true and 0 means false.

For eg, if a cat image was to be identified, a 1 would mean it is a cat image and a 0 would mean it is not a cat image.

For training, lets say our data set includes m cases and each case has nx data, then our input is a matrix of dimensions (nx, m) where every case is its own column. A matix of dimension (m, nx) can also be used but the former is easier to implement
 
![Input matrix](/Notes/2.%20Logistic%20Regression/input%20matrix.png "Input matrix")

For output, we need a single output for each case {0,1} hence our output set will be a matrix of dimension (1,m).
 
![Ouput Matrix](/Notes/2.%20Logistic%20Regression/output%20matrix.png)

## Logistic Regression

The output tells us the probability of a certain case. Therefore, the out y^ lies between 0 and 1.

We can use logistic regression only but that doesn't necessarily give us values between 0 and 1. Hence, we also make use of the sigmoid function. It takes input of any real number and maps it between 0 and 1.

![Sigmoid Function](/Notes/2.%20Logistic%20Regression/Sigmoid%20function.png"Sigmoid Function")

The function used for this is as follows

![y^ Function](/Notes/2.%20Logistic%20Regression/function%20used.png)

## Loss Function

We need to define a loss function that gives a measure of how good our network is. Its parameters are the output expected and the output recieved.    L(y^,y).

We can use the squared difference but for easier optimization, we use the following loss function

**L(y^,y) = -[ylog(y^)+(1-y)log(1-y^)]**

The loss function is defined for one trainig example but we need a measure of how well the netwrok performs for all the training examples. Hence, we define a **Cost Function**.

![Cost function](/Notes/2.%20Logistic%20Regression/cost%20function.png "Cost function")

We want to fine the w,b for which the cost function is minimum.

## Gradient Descent

Our cost function using log has made the function a convex function. On graphing, we get a single bowl. We get a single minima instead of multiple local minima.

![Cost function graph](/Notes/2.%20Logistic%20Regression/cost%20function%20graph.png "COst function Graph")

The way gradient descent works is that we take a random value of w and b and then we step down in the steepest downhil direction. After multiple iterations we reach the global optimum or atleast close to it.

![Gradient Descent](/Notes/2.%20Logistic%20Regression/Gradient%20descent.png "Gradient Descent")


**Recap:**

![Recap](/Notes/2.%20Logistic%20Regression/Logistic%20regression%20formulae.png)

We know that to the network, a weighted sum of the previous data is taken, which is then passed to a sigmoid function, using which we get a Loss function for back propagation.

Now to tune the network, we can change the weights. To minimize the loss function, we need to know how the loss function changes by changing each of the weights. Hence, we find the derivative of the Loss function wrt to each weights. The derivation is given below for one training example.

![Logistic Regression Derivatives](/Notes/2.%20Logistic%20Regression/Logistic%20Regression%20Derivatives.png)

For m examples we take an average as mentioned above (Cost function).

![cost function 2](/Notes/2.%20Logistic%20Regression/cost%20function%202.png)

Similarly, for backpropagation, the derivative of the cost function wrt to each weight is found and then an average of all the derivatives is taken.

![Derivative for m example](/Notes/2.%20Logistic%20Regression/m%20example%20derivative.png)

Algorithm:

![Algorithm step 1](/Notes/2.%20Logistic%20Regression/algorithm%20for%20derivative.png)

![Algorithm step 2](/Notes/2.%20Logistic%20Regression/algorithm%20for%20derivative%20step%202.png)

One iteration of the above steps gives us a single step of gradient descent. All the steps need to be repeated all over again and again for each step of the gradient descent to get us closer and closer to the optima.

The problem with the above method is that it uses 2 for loops for one iteration, one to loop through all the data and second to loop through all the weights. Explicit for loops make the algorithm extremely inefficient and make the program run slower, especially for larger and larger data sets.

There is a set of techniques call **Vectorization** that allow us to get rid of explciit for loops.

