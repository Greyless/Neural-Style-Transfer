

## Bias and Vaariance

![Bias and variance](/Notes/6.%20Tuning/Bias%20And%20variance.png)

![High Bias and High Variance](/Notes/6.%20Tuning/high%20bias%20and%20variance.png)


### How to fix Bias and Variance?

![Steps to Fix](/Notes/6.%20Tuning/steps%20to%20fix.png)


## Regularization

![Regularization](/Notes/6.%20Tuning/Regulariztion.png)

![Regularization 2](/Notes/6.%20Tuning/regularization%202.png)


## Dropout

![Dropout](/Notes/6.%20Tuning/Dropout.png)


## Normalization

![Normalization](/Notes/6.%20Tuning/NOrmalization.png)

If we use unnormalized x, we get a cost function that is more like a swuished out bar. The gradient descent oscillates back and forth a lot so the learning rate has to be very low.

## Vanishing or exploding gradients

When we have a very deep neiral networks with alinear activationg function for each layer, if the weigts of each layer was a little larger than the identity matrix, each subsequent layer would increase the value for Z, this leads to an exponential increase in the value of y^. This is called **Exploding Gradient**.

Similarly, if the weights were a littel smaller than identity matrix, the value for y^ will decrease exponentially. This is called **Variation Gradient**.

This leads to an increase or decrease exponentially for the gradients sent back. this makes training difficult.

### Solution:
 #### Weight initialzation
 In this method we set the initial values of weights close to 1, so the porblem of exploding or vanishing decreses.
 
 ![Weight initialization](/Notes/6.%20Tuning/Weight%20initialization.png)


## Gradient checking

This allows to get an idea for how accurate our logic involving forward propagation adn backpropagation is. If the th evalues of approx d(theta) and actual d(theta) dont match, we can assume that we have a bug somewhere.

![Gradient Checking](/Notes/6.%20Tuning/Gradient%20checking.png)

